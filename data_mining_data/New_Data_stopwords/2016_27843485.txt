 Learning Gaussian graphical model latent variables ill posed insufficient sample complexity, thus appropriately regularized. common choice convex l1 plus nuclear norm regularize searching process. However, best estimator performance always achieved additive convex regularizations, especially sample complexity low. paper, consider concave additive regularization require strong irrepresentable condition. use concave regularization correct intrinsic estimation biases Lasso nuclear penalty well. establish proximity operators concave regularizations, respectively, induces sparsity low rankness. addition, extend method also allow decomposition fused structure-sparsity plus low rankness, providing powerful tool models temporal information. Specifically, develop nontrivial modified alternating direction method multipliers least local convergence. Finally, use synthetic real data validate excellence method. application reconstructing two-stage cancer networks, "the Warburg effect" revealed directly.